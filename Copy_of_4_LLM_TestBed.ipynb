{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FeshkaPelmeshka/INSEAD-AI-Venture-Lab/blob/main/Copy_of_4_LLM_TestBed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To run this notebook, click File -> Save a Copy in Drive\n",
        "\n",
        "After a few seconds, it should open a copy in a new tab for you called Copy of {notebook name}"
      ],
      "metadata": {
        "id": "J1l866tBKDmD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nK8TwdSHz76b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a5b9e47-689c-4507-f078-fbdc826d19c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.104.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "#@title Install Dependencies\n",
        "\n",
        "!pip install openai requests pymupdf pydantic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #@markdown The OpenAI client also works with other providers as well, like Google and Claude, for now we will use it with OpenRouter to get easy access to pretty much all LLMs\n",
        "\n",
        "#@markdown The API key we are using is just for this class. If you want to continue testing after be sure to switch to your own, which you can make here https://openrouter.ai/settings/keys\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=\"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\",\n",
        ")\n",
        "\n",
        "# helper function for later\n",
        "def print_request_info(response, time_elapsed):\n",
        "    # --- Tokens ---\n",
        "    print(\"Provider Used: \", response.provider)\n",
        "    print(\"Prompt tokens:\", response.usage.prompt_tokens)\n",
        "    print(\"Completion tokens:\", response.usage.completion_tokens)\n",
        "    print(\"Total tokens:\", response.usage.total_tokens)\n",
        "\n",
        "    # --- Latency ---\n",
        "    print(\"Request latency (seconds):\", time_elapsed)\n",
        "    print()\n",
        "\n",
        "    # --- Output text ---\n",
        "    print(\"Response:\", response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "VgY794_a7yOR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models you can test out\n",
        "\n",
        "## Closed Source Models\n",
        "\n",
        "- openai/gpt-5\n",
        "- openai/gpt-5-mini\n",
        "- openai/gpt-5-nano\n",
        "- anthropic/claude-sonnet-4\n",
        "- x-ai/grok-4\n",
        "- google/gemini-2.5-flash\n",
        "\n",
        "## Open Source Models\n",
        "- moonshotai/kimi-k2:free\n",
        "- deepseek/deepseek-chat-v3.1:free\n",
        "- z-ai/glm-4.5\n",
        "- qwen/qwen3-235b-a22b-2507\n",
        "\n",
        "\n",
        "These are just a few options you can use. Feel free to check [OpenRouter](https://openrouter.ai/models) and try any of the other models you see there.\n",
        "\n",
        "Many models are offered for free, which make them great candidates for testing and experimenting with LLMs."
      ],
      "metadata": {
        "id": "AO7yeHuD72zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 1. Basic calling\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.time()  # measure request start\n",
        "\n",
        "model_name = \"moonshotai/kimi-k2:free\" # @param {\"type\":\"string\",\"placeholder\":\"moonshotai/kimi-k2:free\"}\n",
        "system_prompt = \"You are a helpful assistant named Andrew.\" # @param {\"type\":\"string\",\"placeholder\":\"You are a helpful assistant named Andrew.\"}\n",
        "user_message = \"Who are you?\" # @param {\"type\":\"string\",\"placeholder\":\"Who are you?\"}\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_message}]\n",
        ")\n",
        "\n",
        "end = time.time()  # measure request end\n",
        "\n",
        "print_request_info(response, end-start)\n"
      ],
      "metadata": {
        "id": "SBOpRFkw74iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Image inputs\n",
        "\n",
        "You will often will want to include images in your request, which means you will need to use a VLM (Vision Language Model). These are LLMs with the added ability to see images.\n",
        "\n",
        "For our use case right now, we will convert a pdf into an image so the AI can see what is in it. It is surprisingly non trivial to convert a PDF into text, so many people will instead pass it as an image like we are here.\n",
        "\n",
        "You will see now that inputting entire PDFs will cause our input tokens to raise dramtically. As you are testing this, think of ways that you could potentially reduce the amount of input tokens needed.\n",
        "\n",
        "## Vision models\n",
        "**bold text**\n",
        "Here is a sampling of VLM's that you can try.\n",
        "\n",
        "### Closed Source Models\n",
        "\n",
        "- openai/gpt-5\n",
        "- openai/gpt-5-mini\n",
        "- openai/gpt-5-nano\n",
        "- anthropic/claude-sonnet-4\n",
        "- x-ai/grok-4\n",
        "- google/gemini-2.5-flash\n",
        "\n",
        "### Open Source Models\n",
        "- z-ai/glm-4.5v\n",
        "- qwen/qwen2.5-vl-32b-instruct\n",
        "\n",
        "You can see that open source is lacking compared to closed source when it comes to VLM options. There are more, but they are rather lack luster, which is why they are not listed here."
      ],
      "metadata": {
        "id": "wbmsXoi0cqAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper function to convert pdfs to images\n",
        "\n",
        "import requests, fitz, base64\n",
        "\n",
        "def pdf_to_imgs(url):\n",
        "    pdf_bytes = requests.get(url).content\n",
        "    with open(\"invoice.pdf\",\"wb\") as f:\n",
        "        f.write(pdf_bytes)\n",
        "\n",
        "    # 2. Convert pages to images (PyMuPDF)\n",
        "    doc = fitz.open(\"invoice.pdf\")\n",
        "    imgs = []\n",
        "    for page in doc:\n",
        "        pix = page.get_pixmap(dpi=200)\n",
        "        imgs.append(\"data:image/png;base64,\" + base64.b64encode(pix.tobytes(\"png\")).decode(\"utf-8\"))\n",
        "\n",
        "    return imgs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TvlHeWK9QZpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"openai/gpt-5-nano\" # @param {\"type\":\"string\",\"placeholder\":\"openai/gpt-5-nano\"}\n",
        "system_prompt = \"Your job is to explain papers and answer questions about them\" # @param {\"type\":\"string\",\"placeholder\":\"Your job is to explain papers and answer questions about them\"}\n",
        "user_message = \"What is the paper's title?\" # @param {\"type\":\"string\",\"placeholder\":\"What is the paper's title?\"}\n",
        "\n",
        "print(\"Converting pdf to images for LLM\")\n",
        "\n",
        "pdf_url = \"https://arxiv.org/pdf/2405.12345.pdf\" # @param {\"type\":\"string\",\"placeholder\":\"https://arxiv.org/pdf/2405.12345.pdf\"}\n",
        "image_urls = pdf_to_imgs(pdf_url)\n",
        "\n",
        "print(\"Done converting images\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [\n",
        "        {\"type\": \"text\", \"text\": user_message},\n",
        "        *[{\"type\": \"image_url\", \"image_url\":{\"url\": img}} for img in image_urls],\n",
        "    ],\n",
        "}]\n",
        "\n",
        "start = time.time()  # measure request start\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "end = time.time()  # measure request end\n",
        "\n",
        "print_request_info(response, end-start)\n"
      ],
      "metadata": {
        "id": "-TkOGCb-KC8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Structured output\n",
        "\n",
        "Often you will want to extract information from a given document and have it converted into an object you can use later in your code in a predictable manner.\n",
        "\n",
        "That is where structred outputs come in. They tell the LLM the format that you want its response to be in, and then the LLM returns its response as JSON in the format you specified, alling for easy parsing so we can use the reponse in the rest of our code.\n",
        "\n",
        "In our exmaple below, we are extracting a users name, DOB, and skills that they have from a natural language response and formatting it so that we can easily go and add it to a database.\n",
        "\n",
        "This is functionality that pretty much all modern LLMs will support, so all of the models that you tried above should work here as well."
      ],
      "metadata": {
        "id": "pqH1U7b-kbA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "model_name = \"openai/gpt-5-nano\" # @param {\"type\":\"string\",\"placeholder\":\"openai/gpt-5-nano\"}\n",
        "system_prompt = \"Your job is to extract the data from the user message and format it into the given response format\" # @param {\"type\":\"string\",\"placeholder\":\"Your job is to explain papers and answer questions about them\"}\n",
        "user_message = \"I'm Lina. I'm 29 and I do Python and product management\" # @param {\"type\":\"string\",\"placeholder\":\"What is the paper's title?\"}\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    name: str\n",
        "    age: Optional[int] = Field(None, ge=0)\n",
        "    skills: List[str] = []\n",
        "\n",
        "start = time.time()\n",
        "response = client.beta.chat.completions.parse(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_message}],\n",
        "    # Just pass the Pydantic class as the format that we want the model to return\n",
        "    response_format=Person\n",
        ")\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print_request_info(response, end-start)"
      ],
      "metadata": {
        "id": "oqQMzpfuZHL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. LLM as a Judge\n",
        "\n",
        "When using models in production, you will want to monitor the model's output, but dont have the time to do it yourself.\n",
        "\n",
        "For this, you can use another LLM to judge the other LLM's outputs. You can give the first LLM's response to a secondary LLM that has a rubric that it will then grade the first LLM's response with.\n",
        "\n",
        "Your rubric can be as simple or as complicated as you want, the simpler the rubric is, the smaller (and cheaper) the model you can use to monitor the outputs. You don't need a super powerful model usually for your judge, as long as your rubric is well defined.\n",
        "\n",
        "We use structered outputs here for the judge so we can easily extract the final score for the model's response."
      ],
      "metadata": {
        "id": "sj0sw4O_mM5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()  # measure request start\n",
        "\n",
        "# INTIAL MESSAGE AND RESPONSE\n",
        "\n",
        "model_one_name = \"moonshotai/kimi-k2:free\" # @param {\"type\":\"string\",\"placeholder\":\"moonshotai/kimi-k2:free\"}\n",
        "model_one_system_prompt = \"You are a helpful assistant named Andrew.\" # @param {\"type\":\"string\",\"placeholder\":\"You are a helpful assistant named Andrew.\"}\n",
        "model_one_user_message = \"Can you explain what mangos are to me and where they are from?\" # @param {\"type\":\"string\",\"placeholder\":\"Who are you?\"}\n",
        "\n",
        "input_messages = [\n",
        "    {\"role\": \"system\", \"content\": model_one_system_prompt},\n",
        "    {\"role\": \"user\", \"content\": model_one_user_message}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=model_one_name,\n",
        "    messages=input_messages\n",
        ")\n",
        "\n",
        "end = time.time()  # measure request end\n",
        "\n",
        "print_request_info(response, end-start)\n",
        "print()\n",
        "\n",
        "# JUDGEMENT\n",
        "\n",
        "class RubricScore(BaseModel):\n",
        "    score: int = Field(ge=0, le=100)\n",
        "\n",
        "start = time.time()  # measure request start\n",
        "\n",
        "judge_model_name = \"openai/gpt-5-nano\" # @param {\"type\":\"string\",\"placeholder\":\"moonshotai/kimi-k2:free\"}\n",
        "judge_system_prompt = \"Your job is to review the given message and everytime you see the word mango (or any variations of it), you should give the model +5 points, up to 100\" # @param {\"type\":\"string\"}\n",
        "\n",
        "judge_messages = [\n",
        "    {\"role\": \"system\", \"content\": judge_system_prompt},\n",
        "    {\"role\": \"user\", \"content\": response.choices[0].message.content}\n",
        "]\n",
        "\n",
        "response = client.beta.chat.completions.parse(\n",
        "    model=model_name,\n",
        "    messages=judge_messages,\n",
        "    response_format=RubricScore\n",
        ")\n",
        "\n",
        "end = time.time()  # measure request end\n",
        "\n",
        "print_request_info(response, end-start)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j7UUX33mkSvu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}